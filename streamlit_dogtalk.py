# streamlit_dogtalk.py
import streamlit as st
import cv2
import numpy as np
from PIL import Image
from ultralytics import YOLO
import pyttsx3

st.set_page_config(page_title="DogTalk AI Interactive MVP", layout="wide")
st.title("ğŸ¶ DogTalk AI å³æ™‚äº’å‹• MVP")

# åˆå§‹åŒ– TTS
engine = pyttsx3.init()
engine.setProperty('rate', 150)  # èªé€Ÿ
engine.setProperty('volume', 1.0)

# è¼‰å…¥ YOLO æ¨¡å‹
@st.cache_resource
def load_model():
    return YOLO("yolov8n.pt")
model = load_model()

# å»ºç«‹ webcam æ•æ‰
st.info("æœ¬åœ°æ¸¬è©¦ç”¨ï¼Œè«‹å…è¨± webcam æ¬Šé™")
cap = cv2.VideoCapture(0)

run = st.checkbox("å•Ÿå‹•å³æ™‚åˆ†æ", value=True)
placeholder = st.empty()

while run:
    ret, frame = cap.read()
    if not ret:
        st.warning("ç„¡æ³•è®€å– webcam")
        break

    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    # YOLO åµæ¸¬ç‹—ç‹—
    results = model.predict(frame_rgb, classes=[16], verbose=False)
    for box in results[0].boxes.xyxy:
        x1, y1, x2, y2 = map(int, box)
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
    
    # å‡è¨­å§¿æ…‹/æƒ…ç·’
    pose = "sit"
    emotion_map = {"sit": "æ”¾é¬†", "stand": "è­¦æˆ’", "lay": "ä¼‘æ¯"}
    emotion = emotion_map.get(pose, "æœªçŸ¥")
    
    # GPT å»ºè­°
    suggestions = {
        "æ”¾é¬†": "ç‰ ç¾åœ¨å¾ˆæ”¾é¬†ï¼Œå¯ä»¥è¼•é¬†äº’å‹•",
        "è­¦æˆ’": "ç‰ æœ‰é»è­¦æˆ’ï¼Œå»ºè­°ä¿æŒè·é›¢",
        "ä¼‘æ¯": "ç‰ åœ¨ä¼‘æ¯ï¼Œè«‹ä¸è¦æ‰“æ“¾"
    }
    suggestion = suggestions.get(emotion, "è§€å¯Ÿç‰ çš„å‹•ä½œ")

    # æ¡†ç·š+æ–‡å­—ç–ŠåŠ 
    cv2.putText(frame, f"æƒ…ç·’: {emotion}", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)
    cv2.putText(frame, f"å»ºè­°: {suggestion}", (10,70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)

    # é¡¯ç¤ºç•«é¢
    placeholder.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

    # æ’­æ”¾èªéŸ³å»ºè­°
    engine.say(suggestion)
    engine.runAndWait()

cap.release()

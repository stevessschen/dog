# streamlit_dogtalk_gtts.py
import streamlit as st
import cv2
import numpy as np
from PIL import Image
from ultralytics import YOLO
from gtts import gTTS
import tempfile
from playsound import playsound
import threading

st.set_page_config(page_title="DogTalk AI MVP", layout="wide")
st.title("ğŸ¶ DogTalk AI å³æ™‚äº’å‹• MVP (Cloud å…¼å®¹ç‰ˆ)")

# è¼‰å…¥ YOLO æ¨¡å‹
@st.cache_resource
def load_model():
    return YOLO("yolov8n.pt")

model = load_model()

# èªéŸ³æ’­æ”¾å‡½æ•¸ï¼ˆä½¿ç”¨ gTTSï¼‰
def speak(text):
    def _play():
        tts = gTTS(text=text, lang='zh-tw')
        with tempfile.NamedTemporaryFile(delete=True, suffix=".mp3") as fp:
            tts.save(fp.name)
            playsound(fp.name)
    threading.Thread(target=_play).start()  # éé˜»å¡

# æœ¬åœ° webcam åŠŸèƒ½ï¼ˆStreamlit Cloud ç„¡æ³•ç›´æ¥ webcamï¼‰
use_webcam = st.checkbox("ä½¿ç”¨ webcam (æœ¬åœ°æ¸¬è©¦)", value=False)

if use_webcam:
    st.warning("è«‹åœ¨æœ¬åœ°åŸ·è¡Œ Streamlit ä»¥ä½¿ç”¨ webcam")
    cap = cv2.VideoCapture(0)
    placeholder = st.empty()
    run = st.checkbox("å•Ÿå‹•å³æ™‚åˆ†æ", value=True)

    while run:
        ret, frame = cap.read()
        if not ret:
            st.warning("ç„¡æ³•è®€å– webcam")
            break

        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = model.predict(frame_rgb, classes=[16], verbose=False)

        for box in results[0].boxes.xyxy:
            x1, y1, x2, y2 = map(int, box)
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

        pose = "sit"  # ç°¡åŒ–ç¤ºç¯„
        emotion_map = {"sit": "æ”¾é¬†", "stand": "è­¦æˆ’", "lay": "ä¼‘æ¯"}
        emotion = emotion_map.get(pose, "æœªçŸ¥")

        suggestions = {
            "æ”¾é¬†": "ç‰ ç¾åœ¨å¾ˆæ”¾é¬†ï¼Œå¯ä»¥è¼•é¬†äº’å‹•",
            "è­¦æˆ’": "ç‰ æœ‰é»è­¦æˆ’ï¼Œå»ºè­°ä¿æŒè·é›¢",
            "ä¼‘æ¯": "ç‰ åœ¨ä¼‘æ¯ï¼Œè«‹ä¸è¦æ‰“æ“¾"
        }
        suggestion = suggestions.get(emotion, "è§€å¯Ÿç‰ çš„å‹•ä½œ")

        cv2.putText(frame, f"æƒ…ç·’: {emotion}", (10,30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)
        cv2.putText(frame, f"å»ºè­°: {suggestion}", (10,70),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)

        placeholder.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

        speak(suggestion)

    cap.release()
else:
    # ä¸Šå‚³åœ–ç‰‡æ¨¡å¼
    uploaded_file = st.file_uploader("ä¸Šå‚³ç‹—ç‹—åœ–ç‰‡", type=["jpg","png"])
    if uploaded_file:
        image = Image.open(uploaded_file)
        st.image(image, caption="åŸå§‹åœ–ç‰‡", use_column_width=True)

        img_np = np.array(image)
        results = model.predict(img_np, classes=[16], verbose=False)

        if len(results[0].boxes) == 0:
            st.warning("æ‰¾ä¸åˆ°ç‹—ç‹—")
        else:
            for box in results[0].boxes.xyxy:
                x1, y1, x2, y2 = map(int, box)
                cv2.rectangle(img_np, (x1, y1), (x2, y2), (0, 255, 0), 2)

            pose = "sit"
            emotion_map = {"sit": "æ”¾é¬†", "stand": "è­¦æˆ’", "lay": "ä¼‘æ¯"}
            emotion = emotion_map.get(pose, "æœªçŸ¥")

            suggestions = {
                "æ”¾é¬†": "ç‰ ç¾åœ¨å¾ˆæ”¾é¬†ï¼Œå¯ä»¥è¼•é¬†äº’å‹•",
                "è­¦æˆ’": "ç‰ æœ‰é»è­¦æˆ’ï¼Œå»ºè­°ä¿æŒè·é›¢",
                "ä¼‘æ¯": "ç‰ åœ¨ä¼‘æ¯ï¼Œè«‹ä¸è¦æ‰“æ“¾"
            }
            suggestion = suggestions.get(emotion, "è§€å¯Ÿç‰ çš„å‹•ä½œ")

            st.image(img_np, caption="åµæ¸¬çµæœ", use_column_width=True)
            st.success(f"æƒ…ç·’: {emotion}")
            st.info(f"å»ºè­°: {suggestion}")
            speak(suggestion)
